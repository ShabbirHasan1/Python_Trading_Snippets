{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the following code to import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:32:13.084366Z",
     "start_time": "2019-06-19T21:32:13.070414Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import quandl\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Running the next code block downloads Gold prices for years 2000-2011 from Quandl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:32:14.606167Z",
     "start_time": "2019-06-19T21:32:13.711064Z"
    }
   },
   "outputs": [],
   "source": [
    "# authentication\n",
    "quandl_key = '{key}' # replace {key} with your own API key  \n",
    "quandl.ApiConfig.api_key = quandl_key\n",
    "\n",
    "# download gold prices from Quandl\n",
    "df = quandl.get(dataset='WGC/GOLD_MONAVG_USD',\n",
    "                start_date='2000-01-01', \n",
    "                end_date='2011-12-31')\n",
    "print(f'Shape of DataFrame: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the next code block, we add rolling statistics (mean, standard deviation) to see how they look like over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:32:18.211856Z",
     "start_time": "2019-06-19T21:32:16.235894Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "df = df.resample(\"M\").last()\n",
    "df.rename(columns={'Value': 'gold_price'}, inplace=True)\n",
    "df['rolling_mean'] = df.gold_price.rolling(window=12).mean()\n",
    "df['rolling_std'] = df.gold_price.rolling(window=12).std()\n",
    "df.plot(title='Gold Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. That is why we decide to use the multiplicative model when doing seasonal decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:20:40.962963Z",
     "start_time": "2019-06-19T21:20:39.978096Z"
    }
   },
   "outputs": [],
   "source": [
    "decomposition_results = seasonal_decompose(df.gold_price, model='multiplicative')\n",
    "decomposition_results.plot().suptitle('Multiplicative Decomposition', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing time series using Facebook's Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the following block to import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T13:13:14.539108Z",
     "start_time": "2019-06-20T13:13:11.790107Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import quandl\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the following block we download daily gold prices from Quandl and divide the series into training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T13:13:15.595233Z",
     "start_time": "2019-06-20T13:13:14.575065Z"
    }
   },
   "outputs": [],
   "source": [
    "# authentication\n",
    "quandl_key = '{key}' # replace {key} with your own API key  \n",
    "quandl.ApiConfig.api_key = quandl_key\n",
    "\n",
    "df = quandl.get(dataset='WGC/GOLD_DAILY_USD',\n",
    "                start_date='2000-01-01',\n",
    "                end_date='2005-12-31')\n",
    "print(f'Shape of DataFrame: {df.shape}')\n",
    "\n",
    "# rename columns\n",
    "df.reset_index(drop=False, inplace=True)\n",
    "df.rename(columns={'Date': 'ds', 'Value': 'y'}, inplace=True)\n",
    "\n",
    "# train-test split\n",
    "df_train = df.loc[df.ds.apply(lambda x: x.year) < 2005].dropna()\n",
    "df_test = df.loc[df.ds.apply(lambda x: x.year) == 2005].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The next block creates the instance of the model and fits it to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T13:13:17.259042Z",
     "start_time": "2019-06-20T13:13:15.635827Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up and fit model \n",
    "model_prophet = Prophet(seasonality_mode='additive')\n",
    "model_prophet.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "model_prophet = model_prophet.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the following code to forecast 1 year ahead and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T13:13:26.889775Z",
     "start_time": "2019-06-20T13:13:22.119338Z"
    }
   },
   "outputs": [],
   "source": [
    "df_future = model_prophet.make_future_dataframe(periods=365)\n",
    "df_pred = model_prophet.predict(df_future)\n",
    "model_prophet.plot(df_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In the next step we inspect the decomposition of the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T20:06:32.799215Z",
     "start_time": "2019-06-18T20:06:30.003866Z"
    }
   },
   "outputs": [],
   "source": [
    "model_prophet.plot_components(df_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Lastly, we want to compare the forecasts to actual data in order to evaluate how the model performed. The following code merges the test set with the forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T13:13:34.062343Z",
     "start_time": "2019-06-20T13:13:34.043668Z"
    }
   },
   "outputs": [],
   "source": [
    "# define outside for readability\n",
    "row_filter = df_pred.ds.apply(lambda x: x.year) == 2005\n",
    "selected_columns = ['ds', 'yhat_lower', 'yhat_upper', 'yhat']\n",
    "\n",
    "df_pred = df_pred.loc[row_filter, selected_columns].reset_index(drop=True)\n",
    "df_test = df_test.merge(df_pred, on=['ds'], how='left')\n",
    "df_test.ds = pd.to_datetime(df_test.ds)\n",
    "df_test.set_index('ds', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Running the following code plots the two series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T13:13:37.361569Z",
     "start_time": "2019-06-20T13:13:34.999989Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax = sns.lineplot(data=df_test[['y', 'yhat_lower', 'yhat_upper', 'yhat']])\n",
    "ax.fill_between(df_test.index,\n",
    "                df_test.yhat_lower,\n",
    "                df_test.yhat_upper,\n",
    "                alpha=0.3)\n",
    "# plot labels\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Gold Price ($)')\n",
    "plt.title('Gold Price - actual vs. predicted', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the first block we iterate over the list of considered values for the hyperparameter, fit the model and store the predictions in a separate `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:47:37.547335Z",
     "start_time": "2019-06-19T20:47:26.258814Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selected changepoints to consider\n",
    "changepoint_priors = [0.01, 0.15]\n",
    "\n",
    "# fit model for all changepoints and store predictions\n",
    "for i, prior in enumerate(changepoint_priors):\n",
    "\n",
    "    model_prophet = Prophet(changepoint_prior_scale=prior)\n",
    "    model_prophet.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "    model_prophet = model_prophet.fit(df_train)\n",
    "\n",
    "    # predict 1 year ahead\n",
    "    df_future = model_prophet.make_future_dataframe(periods=365)\n",
    "        \n",
    "    if i == 0:\n",
    "        df_pred = df_future.copy()\n",
    "\n",
    "    df_future = model_prophet.predict(df_future)\n",
    "\n",
    "    df_pred[f'yhat_upper_{prior}'] = df_future['yhat_upper']\n",
    "    df_pred[f'yhat_lower_{prior}'] = df_future['yhat_lower']\n",
    "    df_pred[f'yhat_{prior}'] = df_future['yhat']\n",
    "\n",
    "# merge back to df to remove weekends\n",
    "df = df.merge(df_pred, on=['ds'], how='left')\n",
    "df.ds = pd.to_datetime(df.ds)\n",
    "df.set_index('ds', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In this step we plot the results and compare the effects of different values of `changepoint_prior_scale`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:47:40.205680Z",
     "start_time": "2019-06-19T20:47:38.138353Z"
    }
   },
   "outputs": [],
   "source": [
    "# selected colors\n",
    "colors = ['b', 'g', 'r', 'c']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# plot actual gold price\n",
    "ax.plot(df.index, df['y'], 'k-', label='actual')\n",
    "\n",
    "# plot results of changepoint analysis\n",
    "for i, prior in enumerate(changepoint_priors):\n",
    "\n",
    "    ax.plot(df.index, df[f'yhat_{prior}'], linewidth=1.2, color=colors[i], label=f'{prior}')\n",
    "\n",
    "    ax.fill_between(df.index, \n",
    "                    df[f'yhat_upper_{prior}'],\n",
    "                    df[f'yhat_lower_{prior}'],\n",
    "                    facecolor=colors[i],\n",
    "                    alpha=0.3, \n",
    "                    edgecolor='k', \n",
    "                    linewidth=0.6)\n",
    "\n",
    "# plot labels\n",
    "plt.legend(loc=2, prop={'size': 10})\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Gold Price ($)')\n",
    "plt.title('Changepoint Prior Analysis', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Performance evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T20:47:44.878546Z",
     "start_time": "2019-06-19T20:47:44.867459Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "# specify outside for readability\n",
    "train_index = df.index.year < 2005\n",
    "test_index = df.index.year == 2005\n",
    "\n",
    "print(f\"Training set RMSE of the model with changepoint_prior_scale = 0.01: {rmse(df.loc[train_index, 'yhat_0.01'], df[train_index].y)}\")\n",
    "print(f\"Training set RMSE of the model with changepoint_prior_scale = 0.15: {rmse(df.loc[train_index, 'yhat_0.15'], df[train_index].y)}\")\n",
    "print(f\"Test set RMSE of the model with changepoint_prior_scale = 0.01: {rmse(df.loc[test_index, 'yhat_0.01'], df[test_index].y)}\")\n",
    "print(f\"Test set RMSE of the model with changepoint_prior_scale = 0.15: {rmse(df.loc[test_index, 'yhat_0.15'], df[test_index].y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T13:44:02.036226Z",
     "start_time": "2019-06-20T13:43:55.023979Z"
    }
   },
   "outputs": [],
   "source": [
    "# cross validation \n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "df_cv = cross_validation(model_prophet, horizon='365 days')\n",
    "df_metrics = performance_metrics(df_cv)\n",
    "plot_cross_validation_metric(df_cv, metric='mape');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for stationarity in time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We need to import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:17:22.770876Z",
     "start_time": "2019-06-19T21:17:21.854911Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The next code block presents how to define a function for running the ADF test and presenting the results in a human-readable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T15:03:50.765158Z",
     "start_time": "2019-06-19T15:03:50.747457Z"
    }
   },
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    '''Perform Augmented Dickey-Fuller test for stationarity'''\n",
    "    indices = ['Test Statistic', 'p-value', '# of Lags Used', '# of Observations Used']\n",
    "    adf_test = adfuller(series, autolag='AIC')\n",
    "    adf_results = pd.Series(adf_test[0:4], index=indices)\n",
    "    for key, value in adf_test[4].items():\n",
    "        adf_results[f'Critical Value ({key})'] = value\n",
    "    \n",
    "    print('Results of Augmented Dickey-Fuller Test:')\n",
    "    print(adf_results)\n",
    "\n",
    "adf_test(df.gold_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The next block presents a similar function, this time for running the KPSS test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T15:03:53.521118Z",
     "start_time": "2019-06-19T15:03:53.503807Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kpss_test(series, h0_type='c'):\n",
    "    '''Perform KPSS test for stationarity'''\n",
    "    indices = ['Test Statistic', 'p-value', '# of Lags']\n",
    "    kpss_test = kpss(series, regression=h0_type)\n",
    "    kpss_results = pd.Series(kpss_test[0:3], index=indices)\n",
    "    for key, value in kpss_test[3].items():\n",
    "        kpss_results[f'Critical Value ({key})'] = value\n",
    "\n",
    "    print('Results of KPSS Test:')\n",
    "    print(kpss_results)\n",
    "\n",
    "kpss_test(df.gold_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lastly, we show how to create the ACF/PACF plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T15:03:56.799450Z",
     "start_time": "2019-06-19T15:03:56.271714Z"
    }
   },
   "outputs": [],
   "source": [
    "# ACF/PACF plots\n",
    "fig, ax = plt.subplots(2, figsize=(16, 8))\n",
    "plot_acf(df.gold_price, ax=ax[0], lags=40, alpha=0.05)\n",
    "plot_pacf(df.gold_price, ax=ax[1], lags=40, alpha=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T15:27:16.883606Z",
     "start_time": "2019-06-19T15:27:16.865322Z"
    }
   },
   "outputs": [],
   "source": [
    "from pmdarima.arima import ndiffs, nsdiffs\n",
    "\n",
    "print(f\"Suggested number of differences (ADF): {ndiffs(df.gold_price, test='adf')}\")\n",
    "print(f\"Suggested number of differences (KPSS): {ndiffs(df.gold_price, test='kpss')}\")\n",
    "print(f\"Suggested number of differences (PP): {ndiffs(df.gold_price, test='pp')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T15:27:08.667923Z",
     "start_time": "2019-06-19T15:27:08.655175Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Suggested number of differences (OSCB): {nsdiffs(df.gold_price, m=12, test='ocsb')}\")\n",
    "print(f\"Suggested number of differences (CH): {nsdiffs(df.gold_price, m=12, test='ch')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for stationarity in time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the following code to import the libraries (the rest of the libraries is the same as in Recipe 'Testing for stationarity in time series'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:32:58.871030Z",
     "start_time": "2019-06-19T21:32:35.599628Z"
    }
   },
   "outputs": [],
   "source": [
    "import cpi\n",
    "from datetime import date\n",
    "from chapter_3_utils import test_autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The next code block covers deflating the prices (to 2011-12-31 USD values) and plotting the new results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:32:59.778890Z",
     "start_time": "2019-06-19T21:32:58.967250Z"
    }
   },
   "outputs": [],
   "source": [
    "df['dt_index'] = df.index.map(lambda x: x.to_pydatetime().date())\n",
    "df['gold_price_deflated'] = df.apply(lambda x: cpi.inflate(x.gold_price, x.dt_index, date(2011, 12, 31)), axis=1)\n",
    "df[['gold_price', 'gold_price_deflated']].plot(title='Gold Price (deflated)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this block we apply natural logarithm to the deflated price series and plot the new series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:33:15.229221Z",
     "start_time": "2019-06-19T21:33:14.477556Z"
    }
   },
   "outputs": [],
   "source": [
    "df['gold_price_log'] = np.log(df.gold_price_deflated)\n",
    "df['rolling_mean_log'] = df.gold_price_log.rolling(window=12).mean()\n",
    "df['rolling_std_log'] = df.gold_price_log.rolling(window=12).std()\n",
    "df[['gold_price_log', 'rolling_mean_log', 'rolling_std_log']].plot(title='Gold Price (logged)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We use `test_autocorrelation` function to investigate if the series became stationary after applied transformations. The function is a combination of stationarity test presented in Recipe 'Testing for stationarity in time series'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:57:22.854977Z",
     "start_time": "2019-06-19T21:57:20.992844Z"
    }
   },
   "outputs": [],
   "source": [
    "test_autocorrelation(df.gold_price_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In this step we apply differencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:34:39.373331Z",
     "start_time": "2019-06-19T21:34:38.559981Z"
    }
   },
   "outputs": [],
   "source": [
    "df['gold_price_log_diff'] = df.gold_price_log.diff(1)\n",
    "df['rolling_mean_log_diff'] = df.gold_price_log_diff.rolling(window=12).mean()\n",
    "df['rolling_std_log_diff'] = df.gold_price_log_diff.rolling(window=12).std()\n",
    "df[['gold_price_log_diff', 'rolling_mean_log_diff', 'rolling_std_log_diff']].plot(\n",
    "    title='Gold Price (1st diff)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. In this step we once again investigate if the differenced series can be considered stationary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:34:52.201627Z",
     "start_time": "2019-06-19T21:34:50.338384Z"
    }
   },
   "outputs": [],
   "source": [
    "test_autocorrelation(df.gold_price_log_diff.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling time series with exponential smoothing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the first block to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T22:12:17.546980Z",
     "start_time": "2019-06-19T22:12:17.527869Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import yfinance as yf\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Having the downloaded the stock prices into the `df` object, we split the data into a training and testing samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T22:12:20.170329Z",
     "start_time": "2019-06-19T22:12:18.031002Z"
    }
   },
   "outputs": [],
   "source": [
    "df = yf.download('AMZN',\n",
    "                 start='2010-01-01',\n",
    "                 end='2018-06-30',\n",
    "                 adjusted=True,\n",
    "                 progress=False)\n",
    "\n",
    "print(f'Downloaded {df.shape[0]} rows of data.')\n",
    "\n",
    "# aggregating to weekly\n",
    "amzn = df.resample('W').last().rename(columns={'Adj Close': 'adj_close'}).adj_close \n",
    "\n",
    "# train-test split\n",
    "amzn_train = amzn[amzn.index.year < 2018]\n",
    "amzn_test = amzn[amzn.index.year == 2018]\n",
    "\n",
    "# define length of test period\n",
    "test_length = len(amzn_test)\n",
    "\n",
    "# plot the stock prices\n",
    "amzn.plot(title='Amazon Stock Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T12:41:31.857880Z",
     "start_time": "2019-06-12T12:41:31.854100Z"
    }
   },
   "source": [
    "3. In the next block we run 3 Simple Exponential Smoothing models and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T22:32:44.258879Z",
     "start_time": "2019-06-19T22:32:41.983813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple Exponential Smoothing ----\n",
    "\n",
    "amzn.plot(color='gray', \n",
    "          title='Simple Exponential Smoothing',\n",
    "          legend=True,\n",
    "          figsize=[16, 9])\n",
    "\n",
    "fit_1 = SimpleExpSmoothing(amzn_train).fit(smoothing_level=0.2)\n",
    "forecast_1 = fit_1.forecast(test_length).rename(r'$\\alpha=0.2$')\n",
    "forecast_1.plot(color='blue', legend=True)\n",
    "fit_1.fittedvalues.plot(color='blue')\n",
    "\n",
    "fit_2 = SimpleExpSmoothing(amzn_train).fit(smoothing_level=0.5)\n",
    "forecast_2 = fit_2.forecast(test_length).rename(r'$\\alpha=0.5$')\n",
    "forecast_2.plot(color='red', legend=True)\n",
    "fit_2.fittedvalues.plot(color='red')\n",
    "\n",
    "fit_3 = SimpleExpSmoothing(amzn_train).fit()\n",
    "alpha = fit_3.model.params['smoothing_level']\n",
    "forecast_3 = fit_3.forecast(test_length).rename(r'$\\alpha={0:.4f}$'.format(alpha))\n",
    "forecast_3.plot(color='green', legend=True)\n",
    "fit_3.fittedvalues.plot(color='green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In the next step we run 3 configurations of Holt's Smoothing models and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T22:36:17.249760Z",
     "start_time": "2019-06-19T22:36:13.684317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Holt's Smoothing models ----\n",
    "\n",
    "amzn.plot(color='gray',\n",
    "          title=\"Holt's Smoothing models\",\n",
    "          legend=True,\n",
    "          figsize=[16, 9])\n",
    "\n",
    "# Holt's model with linear trend\n",
    "fit_1 = Holt(amzn_train).fit()\n",
    "forecast_1 = fit_1.forecast(test_length).rename(\"Linear trend\")\n",
    "fit_1.fittedvalues.plot(color='blue')\n",
    "forecast_1.plot(color='blue', legend=True)\n",
    "\n",
    "# Holt's model with exponential trend\n",
    "fit_2 = Holt(amzn_train, exponential=True).fit()\n",
    "# equivalent of ExponentialSmoothing(train, trend='mul').fit()\n",
    "forecast_2 = fit_2.forecast(test_length).rename(\"Exponential trend\")\n",
    "fit_2.fittedvalues.plot(color='red')\n",
    "forecast_2.plot(color='red', legend=True)\n",
    "\n",
    "# Holt's model with exponential trend and damping\n",
    "fit_3 = Holt(amzn_train, exponential=False, damped=True).fit(damping_slope=0.99)\n",
    "forecast_3 = fit_3.forecast(test_length).rename(\"Exponential trend (damped)\")\n",
    "fit_3.fittedvalues.plot(color='green')\n",
    "forecast_3.plot(color='green', legend=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T22:54:45.252240Z",
     "start_time": "2019-06-19T22:53:51.173854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Holt-Winter's Seasonal Smoothing ----\n",
    "\n",
    "amzn.plot(color='gray',\n",
    "          title=\"Holt-Winter's Seasonal Smoothing\",\n",
    "          legend=True,\n",
    "          figsize=[16, 9])\n",
    "\n",
    "# Holt-Winter's model with exponential trend\n",
    "fit_1 = ExponentialSmoothing(amzn_train, \n",
    "                             trend=\"mul\", \n",
    "                             seasonal=\"add\", \n",
    "                             seasonal_periods=52).fit()\n",
    "forecast_1 = fit_1.forecast(test_length).rename(\"Seasonal Smoothing\")\n",
    "fit_1.fittedvalues.plot(color='blue')\n",
    "forecast_1.plot(color='blue', legend=True)\n",
    "\n",
    "# Holt-Winter's model with exponential trend and damping\n",
    "fit_2 = ExponentialSmoothing(amzn_train, \n",
    "                             trend=\"mul\", \n",
    "                             seasonal=\"add\", \n",
    "                             seasonal_periods=52, \n",
    "                             damped=True).fit()\n",
    "phi = fit_2.model.params['damping_slope']\n",
    "forecast_2 = fit_2.forecast(test_length).rename(r'$Seasonal Smoothing (damped with \\phi={0:.4f})$'.format(phi))\n",
    "fit_2.fittedvalues.plot(color='red')\n",
    "forecast_2.plot(color='red', legend=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling time series with ARIMA class models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the following code to import necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:49:11.499131Z",
     "start_time": "2019-06-21T11:49:11.481407Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from chapter_3_utils import test_autocorrelation\n",
    "import yfinance as yf\n",
    "import pmdarima as pm\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import scipy.stats as scs\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download the Google stock prices and resample to weekly frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:49:29.435491Z",
     "start_time": "2019-06-21T11:49:28.983296Z"
    }
   },
   "outputs": [],
   "source": [
    "df = yf.download('GOOG',\n",
    "                 start='2015-01-01',\n",
    "                 end='2018-12-31',\n",
    "                 adjusted=True,\n",
    "                 progress=False)\n",
    "\n",
    "print(f'Downloaded {df.shape[0]} rows of data.')\n",
    "\n",
    "# aggregate to weekly\n",
    "goog = df.resample('W').last().rename(columns={'Adj Close': 'adj_close'}).adj_close "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply first differences to prices series and plot them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:49:35.190437Z",
     "start_time": "2019-06-21T11:49:32.531145Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply first differences\n",
    "goog_diff = goog.diff().dropna()\n",
    "\n",
    "# plot both series\n",
    "fig, ax = plt.subplots(2)\n",
    "goog.plot(title = \"Google's stock price\", ax=ax[0])\n",
    "goog_diff.plot(ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test the differenced series for stationarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:49:43.001084Z",
     "start_time": "2019-06-21T11:49:40.747756Z"
    }
   },
   "outputs": [],
   "source": [
    "test_autocorrelation(goog_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Based on the results of the tests, specify the ARIMA model and fit it to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:49:55.468952Z",
     "start_time": "2019-06-21T11:49:55.356835Z"
    }
   },
   "outputs": [],
   "source": [
    "arima = ARIMA(goog, order=(2, 1, 1)).fit(disp=0)\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Prepare a function diagnosing the fit of the model based on its residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:50:06.412385Z",
     "start_time": "2019-06-21T11:50:03.699032Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_diagnostics(arima, time_index=None):\n",
    "    '''Function for diagnosing the fit of an ARIMA model by investigating the residuals '''\n",
    "     \n",
    "    # create placeholder subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "    # residuals over time\n",
    "    time_index = range(len(arima.resid)) if time_index is None else time_index\n",
    "    sns.lineplot(x=time_index, y=arima.resid, ax=ax1)\n",
    "    ax1.set_title('Residuals', fontsize=14)\n",
    "\n",
    "    # distribution of residuals\n",
    "    sns.distplot(arima.resid, hist=True, kde=False, norm_hist=True, ax=ax2)\n",
    "    ax2.set_title('Distribution of residuals', fontsize=14)\n",
    "    r_range = np.linspace(min(arima.resid), max(arima.resid), num=1000)\n",
    "    norm_pdf = scs.norm.pdf(r_range, loc=0, scale=1)\n",
    "    ax2.plot(r_range, norm_pdf, 'g', lw=2, label='N(0,1)')\n",
    "        \n",
    "    # QQ plot\n",
    "    qq = sm.qqplot(arima.resid, line='s', ax=ax3)\n",
    "    ax3.set_title('QQ plot', fontsize=14)\n",
    "\n",
    "    # ACF plot\n",
    "    plot_acf(arima.resid, ax=ax4, lags=40, alpha=0.05)\n",
    "    ax4.set_title('ACF plot', fontsize=14)\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_diagnostics(arima, goog.index[1:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Apply and visualise Ljung-Box's test for no autocorrelation in the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:50:18.012415Z",
     "start_time": "2019-06-21T11:50:16.900575Z"
    }
   },
   "outputs": [],
   "source": [
    "ljung_box_results = acorr_ljungbox(arima.resid)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=[16, 5])\n",
    "sns.scatterplot(x=range(len(ljung_box_results[1])), y=ljung_box_results[1], ax=ax)\n",
    "ax.axhline(0.05, ls='--', c='r')\n",
    "ax.set_title(\"Ljung-Box test's results\", fontsize=14)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('p-value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We start by importing the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:50:29.923877Z",
     "start_time": "2019-06-21T11:50:29.921513Z"
    }
   },
   "outputs": [],
   "source": [
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We run `auto_arima` with the majority of settings set to default values. We only exclude potential seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T11:50:32.658407Z",
     "start_time": "2019-06-21T11:50:32.426755Z"
    }
   },
   "outputs": [],
   "source": [
    "auto_arima = pm.auto_arima(goog, \n",
    "                           error_action='ignore',\n",
    "                           suppress_warnings=True,\n",
    "                           seasonal=False)\n",
    "auto_arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the next step we try to tune the search of the optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T12:55:23.661806Z",
     "start_time": "2019-06-21T12:55:14.355343Z"
    }
   },
   "outputs": [],
   "source": [
    "auto_arima = pm.auto_arima(goog,\n",
    "                           error_action='ignore',\n",
    "                           suppress_warnings=True,\n",
    "                           seasonal=False,\n",
    "                           stepwise=False,\n",
    "                           approximation=False,\n",
    "                           n_jobs=-1)\n",
    "auto_arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting using ARIMA class models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download additional test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:11:54.768974Z",
     "start_time": "2019-06-21T13:11:54.540407Z"
    }
   },
   "outputs": [],
   "source": [
    "df = yf.download('GOOG',\n",
    "                 start='2019-01-01',\n",
    "                 end='2019-03-31',\n",
    "                 adjusted=True,\n",
    "                 progress=False)\n",
    "\n",
    "print(f'Downloaded {df.shape[0]} rows of data.')\n",
    "\n",
    "# aggregating to weekly\n",
    "test = df.resample('W').last().rename(columns={'Adj Close': 'adj_close'}).adj_close "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtain forecasts from the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:41:29.801403Z",
     "start_time": "2019-06-21T13:41:29.793792Z"
    }
   },
   "outputs": [],
   "source": [
    "arima_pred = arima.forecast(len(test))\n",
    "\n",
    "# reshaping into a dataframe\n",
    "arima_pred = [pd.DataFrame(arima_pred[0], columns=['prediction']),\n",
    "              pd.DataFrame(arima_pred[2], columns=['ci_lower', 'ci_upper'])]\n",
    "arima_pred = pd.concat(arima_pred, axis=1).set_index(test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Obtain forecasts from the second model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:47:02.455999Z",
     "start_time": "2019-06-21T13:47:02.448565Z"
    }
   },
   "outputs": [],
   "source": [
    "auto_arima_pred = auto_arima.predict(n_periods=len(test), return_conf_int=True, alpha=0.05)\n",
    "\n",
    "# reshaping into a dataframe\n",
    "auto_arima_pred = [pd.DataFrame(auto_arima_pred[0], columns=['prediction']),\n",
    "                   pd.DataFrame(auto_arima_pred[1], columns=['ci_lower', 'ci_upper'])]\n",
    "auto_arima_pred = pd.concat(auto_arima_pred, axis=1).set_index(test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the results on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:49:25.435681Z",
     "start_time": "2019-06-21T13:49:23.006485Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "# plot the observed stock prices\n",
    "ax = sns.lineplot(data=test, color='k', label = 'Actual')\n",
    "\n",
    "# plot the predictions from ARIMA(2,1,1)\n",
    "ax.plot(arima_pred.prediction, c='g', label = 'ARIMA(2,1,1)')\n",
    "ax.fill_between(arima_pred.index,\n",
    "                arima_pred.ci_lower,\n",
    "                arima_pred.ci_upper,\n",
    "                alpha=0.3, \n",
    "                facecolor='g')\n",
    "\n",
    "# plot the predictions from ARIMA(3,1,2)\n",
    "ax.plot(auto_arima_pred.prediction, c='b', label = 'ARIMA(3,1,2)')\n",
    "ax.fill_between(auto_arima_pred.index,\n",
    "                auto_arima_pred.ci_lower,\n",
    "                auto_arima_pred.ci_upper,\n",
    "                alpha=0.3, \n",
    "                facecolor='b')\n",
    "\n",
    "# plot labels\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title(\"Google's stock price  - actual vs. predicted\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "293.301px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
